# “精度評価” Is All You Need

# はじめに

MLの文脈での「評価」は

- オフラインテスト
- オンラインテスト

に分けられると思います。

今回の対象は前者の「**オフライン**テスト」です。

“精度” ではなく “精度評価”

精度も確かに軽視されていると感じるが、より大切なのは”評価”

# 評価指標

機械学習モデルやそれを扱うシステムの開発時に最適化の対象になるものとして以下の三つが挙げられる

- 目的関数(損失関数)
- 評価指標
- KPIなどのビジネス的な数字

### 目的関数

機械学習モデルの「学習」とは「目的関数を最小化する関数を見つけること」とも言い換えられる。

現段階では最適化アルゴリズムに勾配を利用したものが一般的であるため、多くのニューラルネットの学習では微分可能である必要がある（決定木とか強化学習は別）。

### 評価指標

追うべき数字を定式化しモデルの評価を可能にしたもの。

微分可能である必要はない。課題やビジネスの構造が単純で滑らかな関数で書き下せるのであれば、目的関数と同じものでも構わない。

一般的な指標を使用してもいいし、複雑性が高いのであれば自身で定式化することも考える必要が出てくる。

### KPIなどの数字

向上させることがゴールになるもの

売上でなくても良いと思います。ユーザーにとっての価値（ECのレコメンドで言えばどれぐらい実際に買ってくれるかとか、AIリストならユーザーがその中から実際どれだけ契約まで辿り着いたのか）とか。

ここが改善しないのであればどれだけ最先端のモデル・システムでもビジネス的には価値を生まない。もちろん技術的な挑戦・蓄積という面も見るべきではあると思う。

## なぜ評価指標を置くのか

- そもそも評価指標を置いておらず、本当に価値を提供できているのか不透明
- 指標が適切でないため、開発者側とユーザーとの間で体感に差が出る
- 評価指標が改善する追加開発を行なっても肝心のユーザー体験やKPIが変化しない

## いい評価指標とは

ビジネス空間に存在する課題・タスクから技術的な性能を示す空間への写像が的確に対応していること。

## なぜモデル開発ではなくAIサービスでも重要なのか

ブラックボックスを適切に評価する上で必要不可欠であるから。

機械学習モデルの中でもLLMは特に説明可能性が低く、客観的なビジネス要件への適合度や品質保証の判断基準として評価指標が重要になる。ロジックを組む場合であれば、Aという入力であればBになります、という判断ができるがMLはそうではない。

ベンチマークでの精度向上を謳う新モデルはLLMユーザーの開発したサービス・アプリケーションなどの個別タスクの精度向上を保証するものではない。

汎用的なタスクをこなせるモデルの登場によってより複雑なタスクを任せられるようになった一方で、有用性・倫理観・自然かどうか、のような曖昧な概念を扱う必要が生まれ、構造を定式化したり評価指標を置くことの難易度が上がる。

場合によってはHuman In The LoopやLLM-as-a-Judgeのような形をとることも考えなければならないが、評価のブレやすさやコスト面に問題が残る。定式化した評価指標・HITLやLLM-as-a-Judgeによる評価・それぞれにかかるコストなど、評価プロセスの設計そのものにまで話が膨らむ。

## 落とし穴はたくさん

- モデル改善で産むことができる価値は飽和してくる
    - 精度と価値は線形な関係を持たないことが多い
    - 精度が高くなるほど改善にコストがかかるようになる
- 不適切な代理目的変数を立ててしまう
    - 契約まではデータを取得しづらいから名刺交換を代理目的変数にしよう。。。など

などなど

## サービスとしてMLモデル(LLMなど)を使用する場合、損失としての目的関数を意識することは少なくなる（多くの場合意識する必要すらなくなる）が、LLMの登場により扱うタスクが複雑になっており、適切な評価指標を置く難易度が上がる

---

# 適切な精度評価のためのデータ分割

用意したデータを全てモデル・システムの開発に使用してはいけない。

- 学習用データ
    - Training
    - Validation
- テスト用データ（評価用データ）

## モデル開発・プロンプトチューニングの手順

1. データを分ける
    - 開発用データ
        - Trainingデータ
        - Validationデータ
    - テストデータ
2. モデル開発・プロンプトチューニングを行う
    1. Trainingデータで開発を行う
    2. Validationデータで評価を行う
    
    a, bを繰り返して精度を上げていく
    
3. テストデータで評価を行う

主張の分かれそうなところだが、プロンプトチューニングのようなモデルの重みを変更する必要がないタイプの開発の場合、データが潤沢でないならTraining/Validationデータの分割は最悪しなくてもいいかも？

いずれにせよ、テスト用のデータを開発が始まる時点で用意しておく必要がある。

全体のデータを分割した後、テスト用データは基本的に中身を確認してはいけない。

一方のデータでモデル・システム開発を行い、マスクしたテストデータで評価指標がどのように変化するか確認する。

学習用・開発用データと同じ精度が出る→汎用的なモデル・システム開発ができている

学習用・開発用データと比べて精度が低い→過学習したモデル・開発データに特化しすぎたプロンプトになっている可能性がある

### ML/AI系のプロジェクトが始まったら、まずデータを二つに分ける。テストデータはモデル開発やプロンプトチューニングに用いてはいけない。

適当に分けていいわけではない。

例えば企業関連のデータを二つに分ける場合、片方に大企業が多めに入ってしまった場合、LLMは知識として大企業の情報を保持しているため、Fine-TuningしたモデルやRAGを使用したシステムの適切な評価ができない。

### 評価指標が完璧でも用意したデータ次第で信頼性が損なわれてしまう。

例えば上であげた企業関連データの分割の場合、上場企業とそうでない企業のフラグなどを使用して、それぞれの分布の比率を維持して分割する方法（層化抽出（交差検証でいうとstratified k-fold））使用するなどの対策方法があります。

Trainig/Validationデータの分割方法を意味しますが考え方は同じなので、「交差検証」などで調べるとデータの分割方法について学ぶことができると思います。

**さらにテストデータでの評価をあまり繰り返してしまうと、意図せずテストデータへの過適合を起こすこともよく問題になる。データが潤沢にあるかなどを考慮しながら、厳格かつ柔軟に判断する必要がある。**

# 評価指標は機械学習・データ分析をするから必要なわけではない。
ブラックボックスが含まれる処理を客観的に評価するために必要なのであって、MLモデルはブラックボックスの典型的な例であるというだけ。

# ブラックボックスを評価する力が大切！

例え話としてのソフトウェアベンチ